{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8757688,"sourceType":"datasetVersion","datasetId":5261329},{"sourceId":8761226,"sourceType":"datasetVersion","datasetId":5263922},{"sourceId":8761622,"sourceType":"datasetVersion","datasetId":5264144}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"python_version = \"3.9\"","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:02:54.084122Z","iopub.execute_input":"2024-06-23T11:02:54.084504Z","iopub.status.idle":"2024-06-23T11:02:54.096282Z","shell.execute_reply.started":"2024-06-23T11:02:54.084472Z","shell.execute_reply":"2024-06-23T11:02:54.095272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env_name = \"name\"","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:02:54.098199Z","iopub.execute_input":"2024-06-23T11:02:54.098651Z","iopub.status.idle":"2024-06-23T11:02:54.107339Z","shell.execute_reply.started":"2024-06-23T11:02:54.098617Z","shell.execute_reply":"2024-06-23T11:02:54.106532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda create -n {env_name} python={python_version} pip -y","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:02:54.109066Z","iopub.execute_input":"2024-06-23T11:02:54.109463Z","iopub.status.idle":"2024-06-23T11:03:41.272832Z","shell.execute_reply.started":"2024-06-23T11:02:54.109397Z","shell.execute_reply":"2024-06-23T11:03:41.271975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda clean -ya","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:03:41.274802Z","iopub.execute_input":"2024-06-23T11:03:41.275108Z","iopub.status.idle":"2024-06-23T11:04:10.959152Z","shell.execute_reply.started":"2024-06-23T11:03:41.275081Z","shell.execute_reply":"2024-06-23T11:04:10.958081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:04:10.960875Z","iopub.execute_input":"2024-06-23T11:04:10.961178Z","iopub.status.idle":"2024-06-23T11:04:10.966371Z","shell.execute_reply.started":"2024-06-23T11:04:10.961151Z","shell.execute_reply":"2024-06-23T11:04:10.964472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"old_path = os.environ[\"PATH\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:04:10.969146Z","iopub.execute_input":"2024-06-23T11:04:10.969539Z","iopub.status.idle":"2024-06-23T11:04:10.984558Z","shell.execute_reply.started":"2024-06-23T11:04:10.969505Z","shell.execute_reply":"2024-06-23T11:04:10.983751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_path = f\"/opt/conda/envs/{env_name}/bin:{old_path}\"","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:04:10.985668Z","iopub.execute_input":"2024-06-23T11:04:10.985979Z","iopub.status.idle":"2024-06-23T11:04:10.995173Z","shell.execute_reply.started":"2024-06-23T11:04:10.985946Z","shell.execute_reply":"2024-06-23T11:04:10.994342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env PATH=$new_path","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:04:10.996375Z","iopub.execute_input":"2024-06-23T11:04:10.996715Z","iopub.status.idle":"2024-06-23T11:04:11.006424Z","shell.execute_reply.started":"2024-06-23T11:04:10.996687Z","shell.execute_reply":"2024-06-23T11:04:11.005489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python --version","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:04:11.007736Z","iopub.execute_input":"2024-06-23T11:04:11.007985Z","iopub.status.idle":"2024-06-23T11:04:11.953515Z","shell.execute_reply.started":"2024-06-23T11:04:11.007964Z","shell.execute_reply":"2024-06-23T11:04:11.952624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Download ollama\n!curl -fsSL https://ollama.com/install.sh | sh\n\n# To support background processes in Kaggle\nimport os\nget_ipython().system = os.system\n\n!ollama serve &\n\n# Explore more ollama models here: https://ollama.com/library\n!ollama pull zephyr","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:04:11.954858Z","iopub.execute_input":"2024-06-23T11:04:11.955169Z","iopub.status.idle":"2024-06-23T11:04:15.297984Z","shell.execute_reply.started":"2024-06-23T11:04:11.955142Z","shell.execute_reply":"2024-06-23T11:04:15.296710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore more ollama models here: https://ollama.com/library\n!ollama pull mistral","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:04:15.299806Z","iopub.execute_input":"2024-06-23T11:04:15.300867Z","iopub.status.idle":"2024-06-23T11:04:54.372808Z","shell.execute_reply.started":"2024-06-23T11:04:15.300831Z","shell.execute_reply":"2024-06-23T11:04:54.371850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ollama pull llama3","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:04:54.375720Z","iopub.execute_input":"2024-06-23T11:04:54.376003Z","iopub.status.idle":"2024-06-23T11:05:32.580998Z","shell.execute_reply.started":"2024-06-23T11:04:54.375979Z","shell.execute_reply":"2024-06-23T11:05:32.579999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ollama pull qwen2","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:05:32.582217Z","iopub.execute_input":"2024-06-23T11:05:32.582555Z","iopub.status.idle":"2024-06-23T11:06:10.839324Z","shell.execute_reply.started":"2024-06-23T11:05:32.582527Z","shell.execute_reply":"2024-06-23T11:06:10.838384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ollama pull llama3:instruct","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:06:10.840848Z","iopub.execute_input":"2024-06-23T11:06:10.841606Z","iopub.status.idle":"2024-06-23T11:06:11.216286Z","shell.execute_reply.started":"2024-06-23T11:06:10.841569Z","shell.execute_reply":"2024-06-23T11:06:11.215467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt-get install espeak-ng -y","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-23T11:06:11.217642Z","iopub.execute_input":"2024-06-23T11:06:11.217959Z","iopub.status.idle":"2024-06-23T11:06:17.749246Z","shell.execute_reply.started":"2024-06-23T11:06:11.217909Z","shell.execute_reply":"2024-06-23T11:06:17.748362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q tts accelerate moviepy diffusers langchain pydub openai-whisper ffmpeg-python Pillow","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:06:17.750893Z","iopub.execute_input":"2024-06-23T11:06:17.751190Z","iopub.status.idle":"2024-06-23T11:11:19.623976Z","shell.execute_reply.started":"2024-06-23T11:06:17.751165Z","shell.execute_reply":"2024-06-23T11:11:19.622927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install langchain-community","metadata":{"execution":{"iopub.status.busy":"2024-06-23T11:11:19.625390Z","iopub.execute_input":"2024-06-23T11:11:19.625792Z","iopub.status.idle":"2024-06-23T11:11:26.225181Z","shell.execute_reply.started":"2024-06-23T11:11:19.625756Z","shell.execute_reply":"2024-06-23T11:11:26.224276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile main.py\nimport torch\nfrom pydub import AudioSegment\nfrom diffusers import StableDiffusionXLPipeline, DPMSolverSinglestepScheduler\nfrom langchain_community.llms import Ollama\nfrom langchain import PromptTemplate, LLMChain\nfrom moviepy.editor import *\nfrom TTS.api import TTS\nimport whisper\nimport os\nimport json\nimport ffmpeg\nfrom PIL import Image, ImageDraw, ImageFont\n\n# Initialize models and pipeline\ndef initialize_models():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Load stable diffusion pipeline\n    pipe = StableDiffusionXLPipeline.from_pretrained(\"sd-community/sdxl-flash\", torch_dtype=torch.float16).to(device)\n    pipe.scheduler = DPMSolverSinglestepScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n\n    # Initialize TTS model\n    tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC_ph\", progress_bar=False).to(device)\n\n    return pipe, tts, device\n\npipe, tts, device = initialize_models()\n\n# LLM initialization\ndef get_llm(model_name):\n    print(f\"Building LLM Model: {model_name}\")\n    return Ollama(model=model_name, temperature=0.4)\n\n# Prompt templates\ndef build_template():\n    fact_template = '''\nYou are a talented YouTube content creator working for a famous YouTuber. Your task is to create a 35-second YouTube Short script on \"Top 3 facts about {topic}\" for a wide audience. Start with \"Here are the Top 3 facts about {topic}. The last one will blow your mind.\" Present the topics in descending order from 3rd to 1st. Be creative and engaging.\n\nClose the script by asking viewers to like, subscribe, and share the video.\n\nExample:\n\nTitle: Top 5 Mysterious Facts About Ancient Indian Men\nDescription: Intriguing facts about ancient Indian men.\nVisual: A statue of an ancient Indian man in the city.\nNarration: \"Here are the Top 5 Mysterious Facts About Ancient Indian Men. The last one will blow your mind!\"\nVisual: Ancient Indian men doing yoga, transitioning to a modern yoga class.\nNarration: \"Fact 5: Yoga, practiced by millions today, was created by ancient Indian men thousands of years ago.\"\nVisual: An ancient Indian man observing the night sky, transitioning to a modern planetarium.\nNarration: \"Fact 4: Ancient Indian men were stellar astronomers who calculated the Earth's circumference and theorized about gravity centuries before Western science.\"\nVisual: Ruins of the Indus Valley Civilization with undeciphered scripts.\nNarration: \"Fact 3: The Indus Valley Civilization's script remains undeciphered. Their urban planning and drainage systems are still a mystery.\"\nVisual: Ancient Ayurvedic texts, transitioning to modern herbal medicine.\nNarration: \"Fact 2: Ayurveda, one of the oldest holistic healing systems, was pioneered by ancient Indian men. Their texts detail surgical procedures and herbal medicines.\"\nVisual: Artistic depictions of Vimanas flying, transitioning to ancient scriptures and modern interpretations.\nNarration: \"Fact 1: The Vimana, described in ancient texts as flying machines, suggest advanced technology or extraterrestrial contact.\"\nVisual: YouTube's Like and Share buttons.\nNarration: \"If these facts blew your mind, like, subscribe, and share this video. See you next time!\"\n\nCreate your script following this format. Keep it short, interesting, and simple.\n    '''  # Same as in the original\n    quiz_template = '''\nYou are a skilled YouTube content creator working for a renowned Investigation quiz Creator. Your task is to craft a 55-second YouTube Short script focused on an intriguing investigation quiz on {topic}.\n\nBegin by setting the scene, introduce each suspect individually, and present compelling clues to your audience. Ensure your narration is creative and captivating.\n\nConclude the script by inviting viewers to identify the culprit in the comments, and encourage them to like, subscribe, and share the video.\n\nExample:\n\nTitle: Quick Investigation Quiz: Who Stole the Diamond Necklace?\nDescription: Can you solve the mystery in 55 seconds?\nVisual: A detective's magnifying glass focusing on a diamond necklace.\nNarration: \"Hey detectives! Ready for a quick investigation quiz? Let's see if you can crack the case in 55 seconds!\"\nVisual: A hotel room in disarray, with a broken window illustrating the crime scene.\nNarration: \"Here's the scene: A valuable diamond necklace has gone missing from this hotel room. The room was ransacked, and the window was broken from the inside. There are three suspects who were in the hotel at the time.\"\nVisual: Mr. White, looking concerned.\nNarration: \"Suspect A: Mr. White, the hotel manager, who has access to all rooms.\"\nVisual: Ms. Blue, looking sly.\nNarration: \"Suspect B: Ms. Blue, a famous jewel thief recently spotted in the area.\"\nVisual: Mr. Red, looking nervous.\nNarration: \"Suspect C: Mr. Red, a guest staying in the room next door, known for his shady dealings.\"\nVisual: An illustration of some evidence boxes in the police locker.\nNarration: \"Here are the clues:\"\nVisual: A glove on the floor, with a magnifying glass hovering over it.\nNarration: \"Clue 1: A glove was found on the floor, but it doesn't belong to the room's owner.\"\nVisual: A key card in the hallway.\nNarration: \"Clue 2: A room key card was found in the hallway, registered to Mr. Red.\"\nVisual: Security camera footage showing Ms. Blue entering the hotel.\nNarration: \"Clue 3: A security camera shows Ms. Blue entering the hotel at the time of the theft.\"\nVisual: The three suspects (i.e., Mr. White, Ms. Blue, and Mr. Red) side by side.\nNarration: \"Who do you think stole the diamond necklace? Was it Mr. White, Ms. Blue, or Mr. Red? Comment your answer below!\"\nVisual: YouTube's Like and Share buttons.\nNarration: \"Don't forget to like, share, and subscribe for more investigation quizzes!\"\n\nCraft your script following this structure. Keep it concise, intriguing, and engaging for your audience.\n    '''  # Same as in the original\n    return quiz_template, fact_template\n\n# LLM chain\ndef get_llm_chain(template, llm):\n    print(\"Building LLM Chain with prompt and model\")\n    prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n    return prompt | llm\n\n# Parsing LLM response\ndef parse_llm_response(response):\n    print(\"Parsing response from LLM model\")\n    text_array = response.splitlines()\n\n    title = \"\"\n    description = \"\"\n    visuals = []\n    narrations = []\n\n    for text in text_array:\n        if \"Title:\" in text:\n            title = text.replace(\"Title: \", \"\")\n        elif \"Description:\" in text:\n            description = text.replace(\"Description: \", \"\")\n        elif \"Visual:\" in text:\n            visuals.append(text.replace(\"Visual: \", \"\"))\n        elif \"Narration:\" in text:\n            narrations.append(text.replace(\"Narration: \", \"\"))\n\n    return title, description, visuals, narrations\n\n# Convert text to images\ndef convert_text_to_image(visuals, output_dir='images'):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    print(\"Converting text to images\")\n    image_files = []\n    for count, visual in enumerate(visuals):\n        file_path = os.path.join(output_dir, f'output_image{count}.png')\n        pipe(f\"{visual}, realism\", width=768, height=1344, num_inference_steps=7, guidance_scale=3).images[0].save(file_path)\n        image_files.append(file_path)\n    \n    return image_files\n\n# Change audio speed\ndef change_speed(audio, speed_factor):\n    return audio._spawn(audio.raw_data, overrides={\"frame_rate\": int(audio.frame_rate * speed_factor)}).set_frame_rate(audio.frame_rate)\n\n# Generate speech from text\ndef generate_speech(narrations, speed_factor=1.25, output_dir='speeches'):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    audio_files = []\n    for count, narration in enumerate(narrations):\n        file_name = os.path.join(output_dir, f\"speech{count}.wav\")\n        tts.tts_to_file(text=narration, file_path=file_name)\n        \n        audio = AudioSegment.from_file(file_name)\n        audio = change_speed(audio, speed_factor)\n        audio.export(file_name, format=\"wav\")\n        \n        audio_files.append(file_name)\n    \n    return audio_files\n\n# Generate video\ndef generate_video(image_files, audio_files, output_file):\n    video_clips = []\n    for image, audio in zip(image_files, audio_files):\n        audio_clip = AudioFileClip(audio)\n        img_clip = ImageClip(image).set_duration(audio_clip.duration).set_audio(audio_clip)\n        video_clips.append(img_clip)\n    \n    final_video = concatenate_videoclips(video_clips, method=\"compose\")\n    final_video = final_video.set_fps(24)\n    \n    final_audio = final_video.audio\n    final_audio.fps = 44100\n    audio_file_path = 'output_audio.mp3'\n    final_audio.write_audiofile(audio_file_path)\n    \n    transcribe_audio_with_timestamps(audio_file_path)\n    \n    line_level_timestamps = split_text_into_lines()\n    \n    create_subtitle_images_with_highlight(line_level_timestamps, 768, 1344)\n    \n    final_video = add_highlighted_subtitles(final_video, line_level_timestamps)\n    \n    final_video = generate_video_with_bg_audio(final_video, \"/kaggle/input/prisms/prism.mp3\")\n    \n    final_video.write_videofile(output_file, codec=\"libx264\", audio_codec=\"aac\")\n\n\n# Transcribe audio with timestamps\ndef transcribe_audio_with_timestamps(audiofilename):\n    model = whisper.load_model(\"medium\", device=device)\n    result = model.transcribe(audiofilename, word_timestamps=True)\n    \n    wordlevel_info = [{'word': word['word'].strip(), 'start': word['start'], 'end': word['end']} for segment in result['segments'] for word in segment['words']]\n    \n    with open('data.json', 'w') as f:\n        json.dump(wordlevel_info, f, indent=4)\n\n# Split text into lines for subtitles\ndef split_text_into_lines():\n    with open('data.json', 'r') as f:\n        data = json.load(f)\n    \n    MaxChars = 10 \n    MaxDuration = 1\n    MaxGap = 1.5\n\n    subtitles = []\n    line = []\n    line_duration = 0\n\n    for idx, word_data in enumerate(data):\n        word, start, end = word_data[\"word\"], word_data[\"start\"], word_data[\"end\"]\n        line.append(word_data)\n        line_duration += end - start\n        temp = \" \".join(item[\"word\"] for item in line)\n\n        duration_exceeded = line_duration > MaxDuration \n        chars_exceeded = len(temp) > MaxChars \n        maxgap_exceeded = idx > 0 and word_data['start'] - data[idx-1]['end'] > MaxGap\n\n        if duration_exceeded or chars_exceeded or maxgap_exceeded:\n            if line:\n                subtitles.append({\n                    \"word\": \" \".join(item[\"word\"] for item in line),\n                    \"start\": line[0][\"start\"],\n                    \"end\": line[-1][\"end\"],\n                    \"textcontents\": line\n                })\n                line = []\n                line_duration = 0\n\n    if line:\n        subtitles.append({\n            \"word\": \" \".join(item[\"word\"] for item in line),\n            \"start\": line[0][\"start\"],\n            \"end\": line[-1][\"end\"],\n            \"textcontents\": line\n        })\n\n    return subtitles\n\n# Create subtitle images\ndef create_subtitle_image(text, current_word_idx, width, height, font_path='/kaggle/input/boldttf/Roboto-Bold.ttf', font_size=80):\n    img = Image.new('RGBA', (width, height), (0, 0, 0, 0))\n    draw = ImageDraw.Draw(img)\n    \n    try:\n        font = ImageFont.truetype(font_path, font_size) if font_path else ImageFont.load_default()\n    except Exception as e:\n        print(f\"Error loading font: {e}. Using default font.\")\n        font = ImageFont.load_default()\n    \n    words = text.split()\n    total_width = sum([draw.textbbox((0, 0), word, font=font)[2] for word in words])\n    x_offset = (width - total_width) // 2\n    \n    outline_offset = 2\n\n    for idx, word in enumerate(words):\n        fill = (255, 0, 0, 255) if idx == current_word_idx else (255, 255, 255, 255)\n        word_bbox = draw.textbbox((0, 0), word, font=font)\n        word_width = word_bbox[2] - word_bbox[0]\n        word_height = word_bbox[3] - word_bbox[1]\n\n        for x in range(-outline_offset, outline_offset + 1):\n            for y in range(-outline_offset, outline_offset + 1):\n                if x != 0 or y != 0:\n                    draw.text((x_offset + x, (height - word_height) // 2 + y), word, font=font, fill=(0, 0, 0, 255))\n        \n        draw.text((x_offset, (height - word_height) // 2), word, font=font, fill=fill)\n        x_offset += word_width + draw.textbbox((0, 0), ' ', font=font)[2]\n    \n    return img\n\ndef create_subtitle_images_with_highlight(linelevel_timestamps, width, height, output_dir='subtitles'):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    for i, line in enumerate(linelevel_timestamps):\n        subtitle = line['word']\n        for j, content in enumerate(line['textcontents']):\n            img = create_subtitle_image(subtitle, j, width, height)\n            img_path = os.path.join(output_dir, f\"subtitle_{i}_{j}.png\")\n            img.save(img_path)\n\n# Add highlighted subtitles to video\ndef add_highlighted_subtitles(video, linelevel_timestamps, input_dir='subtitles'):\n    subclips = []\n\n    for i, line in enumerate(linelevel_timestamps):\n        for j, content in enumerate(line['textcontents']):\n            img_path = os.path.join(input_dir, f\"subtitle_{i}_{j}.png\")\n            if not os.path.exists(img_path):\n                print(f\"Subtitle image {img_path} does not exist.\")\n                continue\n\n            img_clip = ImageClip(img_path).set_start(content['start']).set_duration(content['end'] - content['start']).set_position((\"center\", \"bottom\"))\n            subclips.append(img_clip)\n\n    video_with_subs = CompositeVideoClip([video, *subclips])\n    return video_with_subs\n\ndef generate_video_with_bg_audio(video_clip, bg_file_path):\n    # Path to your audio file\n    audio_path = bg_file_path\n    \n    # Duration of the video clip in seconds\n    video_duration = video_clip.duration\n    \n    # Load audio clip\n    audio_clip = AudioFileClip(audio_path)\n    \n    # Start the audio from 25 seconds and end at the end of the video clip\n    start_time = 25  # seconds\n    end_time = start_time + video_duration  # seconds\n    \n    # Trim the audio clip\n    trimmed_audio = audio_clip.subclip(start_time, end_time)\n    \n    # Set the volume of the background music (audio from audio file)\n    background_music_volume = 0.15  # You can adjust this value as needed\n    \n    # Set the volume of the original video's audio\n    original_audio_volume = 1.0  # Full volume (1.0)\n    \n    # Set the volume for the background music\n    background_music = trimmed_audio.volumex(background_music_volume)\n    \n    # Get original video's audio\n    original_audio = video_clip.audio\n    \n    # Reduce the volume of the original video's audio\n    reduced_original_audio = original_audio.volumex(original_audio_volume - background_music_volume)\n    \n    # Combine the reduced original audio with the background music\n    final_audio = CompositeAudioClip([reduced_original_audio, background_music])\n    \n    # Set the audio of the video clip to the combined audio\n    video_clip = video_clip.set_audio(final_audio)\n    return video_clip\n\n# Main function to generate video with subtitles and background music\ndef main(model_name, niche, topic):\n    llm = get_llm(model_name)\n    \n    quiz_template, fact_template = build_template()\n    template = quiz_template if niche == \"quiz\" else fact_template\n    \n    llm_chain = get_llm_chain(template, llm)\n\n    response = llm_chain.invoke(topic)\n    title, description, visuals, narrations = parse_llm_response(response)\n\n    image_files = convert_text_to_image(visuals)\n    audio_files = generate_speech(narrations)\n\n    output_file = 'output_video.mp4'\n    generate_video(image_files, audio_files, output_file)\n\n# Execute main function\nif __name__ == \"__main__\":\n    model_name, niche, topic = sys.argv[1:]\n    main(model_name, niche, topic)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:10:57.841111Z","iopub.execute_input":"2024-06-23T12:10:57.841525Z","iopub.status.idle":"2024-06-23T12:10:57.857505Z","shell.execute_reply.started":"2024-06-23T12:10:57.841493Z","shell.execute_reply":"2024-06-23T12:10:57.856639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python ./main.py \"llama3:instruct\" \"fact\" \"Kohinoor\"","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:11:14.316522Z","iopub.execute_input":"2024-06-23T12:11:14.316880Z","iopub.status.idle":"2024-06-23T12:14:24.376574Z","shell.execute_reply.started":"2024-06-23T12:11:14.316853Z","shell.execute_reply":"2024-06-23T12:14:24.375492Z"},"trusted":true},"execution_count":null,"outputs":[]}]}