{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8757688,"sourceType":"datasetVersion","datasetId":5261329},{"sourceId":8761226,"sourceType":"datasetVersion","datasetId":5263922},{"sourceId":8761622,"sourceType":"datasetVersion","datasetId":5264144}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"python_version = \"3.9\"","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:53:47.401333Z","iopub.execute_input":"2024-06-30T11:53:47.401694Z","iopub.status.idle":"2024-06-30T11:53:47.405946Z","shell.execute_reply.started":"2024-06-30T11:53:47.401665Z","shell.execute_reply":"2024-06-30T11:53:47.405051Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"env_name = \"name\"","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:53:47.407591Z","iopub.execute_input":"2024-06-30T11:53:47.407991Z","iopub.status.idle":"2024-06-30T11:53:47.420100Z","shell.execute_reply.started":"2024-06-30T11:53:47.407965Z","shell.execute_reply":"2024-06-30T11:53:47.419343Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"!conda create -n {env_name} python={python_version} pip -y","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:53:47.421133Z","iopub.execute_input":"2024-06-30T11:53:47.421458Z","iopub.status.idle":"2024-06-30T11:53:48.665188Z","shell.execute_reply.started":"2024-06-30T11:53:47.421418Z","shell.execute_reply":"2024-06-30T11:53:48.664257Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"\nInvalidMatchSpec: Invalid spec 'python={python_version}': Invalid version '{python_version}': invalid character(s)\n\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"256"},"metadata":{}}]},{"cell_type":"code","source":"!conda clean -ya","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:53:48.666535Z","iopub.execute_input":"2024-06-30T11:53:48.667122Z","iopub.status.idle":"2024-06-30T11:54:39.155963Z","shell.execute_reply.started":"2024-06-30T11:53:48.667085Z","shell.execute_reply":"2024-06-30T11:54:39.155088Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"There are no unused tarball(s) to remove.\nThere are no index cache(s) to remove.\nThere are no unused package(s) to remove.\nThere are no tempfile(s) to remove.\nThere are no logfile(s) to remove.\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:54:39.158840Z","iopub.execute_input":"2024-06-30T11:54:39.159394Z","iopub.status.idle":"2024-06-30T11:54:39.163130Z","shell.execute_reply.started":"2024-06-30T11:54:39.159366Z","shell.execute_reply":"2024-06-30T11:54:39.162137Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"old_path = os.environ[\"PATH\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:54:39.164736Z","iopub.execute_input":"2024-06-30T11:54:39.165065Z","iopub.status.idle":"2024-06-30T11:54:39.230461Z","shell.execute_reply.started":"2024-06-30T11:54:39.165035Z","shell.execute_reply":"2024-06-30T11:54:39.229678Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"new_path = f\"/opt/conda/envs/{env_name}/bin:{old_path}\"","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:54:39.231755Z","iopub.execute_input":"2024-06-30T11:54:39.232406Z","iopub.status.idle":"2024-06-30T11:54:39.247103Z","shell.execute_reply.started":"2024-06-30T11:54:39.232377Z","shell.execute_reply":"2024-06-30T11:54:39.246270Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"%env PATH=$new_path","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:54:39.248179Z","iopub.execute_input":"2024-06-30T11:54:39.248442Z","iopub.status.idle":"2024-06-30T11:54:39.262229Z","shell.execute_reply.started":"2024-06-30T11:54:39.248418Z","shell.execute_reply":"2024-06-30T11:54:39.261390Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"env: PATH=/opt/conda/envs/name/bin:/opt/conda/envs/name/bin:/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n","output_type":"stream"}]},{"cell_type":"code","source":"!python --version","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:54:39.263312Z","iopub.execute_input":"2024-06-30T11:54:39.263576Z","iopub.status.idle":"2024-06-30T11:54:39.280632Z","shell.execute_reply.started":"2024-06-30T11:54:39.263543Z","shell.execute_reply":"2024-06-30T11:54:39.279759Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Python 3.9.19\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"#Download ollama\n!curl -fsSL https://ollama.com/install.sh | sh\n\n# To support background processes in Kaggle\nimport os\nget_ipython().system = os.system\n\n!ollama serve &","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:54:39.281800Z","iopub.execute_input":"2024-06-30T11:54:39.282067Z","iopub.status.idle":"2024-06-30T11:54:41.558970Z","shell.execute_reply.started":"2024-06-30T11:54:39.282044Z","shell.execute_reply":"2024-06-30T11:54:41.557813Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":">>> Downloading ollama...\n######################################################################## 100.0%#=#=#                                                                          \n>>> Installing ollama to /usr/local/bin...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n>>> NVIDIA GPU installed.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# Explore more ollama models here: https://ollama.com/library\n!ollama pull mistral:7b-instruct-fp16","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:54:41.560713Z","iopub.execute_input":"2024-06-30T11:54:41.561113Z","iopub.status.idle":"2024-06-30T11:54:47.602455Z","shell.execute_reply.started":"2024-06-30T11:54:41.561079Z","shell.execute_reply":"2024-06-30T11:54:47.601524Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"2024/06/30 11:54:41 routes.go:1064: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\ntime=2024-06-30T11:54:41.575Z level=INFO source=images.go:730 msg=\"total blobs: 5\"\ntime=2024-06-30T11:54:41.576Z level=INFO source=images.go:737 msg=\"total unused blobs removed: 0\"\ntime=2024-06-30T11:54:41.576Z level=INFO source=routes.go:1111 msg=\"Listening on 127.0.0.1:11434 (version 0.1.48)\"\ntime=2024-06-30T11:54:41.577Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama128773846/runners\ntime=2024-06-30T11:54:46.367Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cuda_v11 rocm_v60101 cpu cpu_avx cpu_avx2]\"\ntime=2024-06-30T11:54:46.675Z level=INFO source=types.go:98 msg=\"inference compute\" id=GPU-a4450c2d-4493-10bf-b83f-11e2e3bda0ef library=cuda compute=7.5 driver=12.2 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\ntime=2024-06-30T11:54:46.675Z level=INFO source=types.go:98 msg=\"inference compute\" id=GPU-4d7241af-12cf-978a-036f-896da5cbf0bc library=cuda compute=7.5 driver=12.2 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n\u001b[?25lpulling manifest ⠋ \u001b[?25h","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/06/30 - 11:54:46 | 200 |      56.278µs |       127.0.0.1 | HEAD     \"/\"\n","output_type":"stream"},{"name":"stderr","text":"\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/06/30 - 11:54:47 | 200 |  919.354884ms |       127.0.0.1 | POST     \"/api/pull\"\n","output_type":"stream"},{"name":"stderr","text":"\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \npulling bbd75c7dc429... 100% ▕████████████████▏  14 GB                         \npulling 43070e2d4e53... 100% ▕████████████████▏  11 KB                         \npulling 22e1b2e8dc2f... 100% ▕████████████████▏   43 B                         \npulling ed11eda7790d... 100% ▕████████████████▏   30 B                         \npulling f5989910299d... 100% ▕████████████████▏  482 B                         \nverifying sha256 digest \nwriting manifest \nremoving any unused layers \nsuccess \u001b[?25h\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"!apt-get install espeak-ng -y","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-30T11:54:47.603679Z","iopub.execute_input":"2024-06-30T11:54:47.603988Z","iopub.status.idle":"2024-06-30T11:54:50.684027Z","shell.execute_reply.started":"2024-06-30T11:54:47.603962Z","shell.execute_reply":"2024-06-30T11:54:50.683030Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Reading package lists...\nBuilding dependency tree...\nReading state information...\nespeak-ng is already the newest version (1.50+dfsg-6).\n0 upgraded, 0 newly installed, 0 to remove and 75 not upgraded.\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"!pip install -q tts accelerate moviepy diffusers langchain pydub openai-whisper ffmpeg-python Pillow pytrends","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:54:50.687834Z","iopub.execute_input":"2024-06-30T11:54:50.688127Z","iopub.status.idle":"2024-06-30T11:54:57.291683Z","shell.execute_reply.started":"2024-06-30T11:54:50.688102Z","shell.execute_reply":"2024-06-30T11:54:57.290815Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"!pip install langchain-community","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:54:57.292936Z","iopub.execute_input":"2024-06-30T11:54:57.293220Z","iopub.status.idle":"2024-06-30T11:55:00.716349Z","shell.execute_reply.started":"2024-06-30T11:54:57.293194Z","shell.execute_reply":"2024-06-30T11:55:00.715401Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain-community in /opt/conda/envs/name/lib/python3.9/site-packages (0.2.6)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain-community) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain-community) (2.0.31)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain-community) (3.9.5)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: langchain<0.3.0,>=0.2.6 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain-community) (0.2.6)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain-community) (0.2.10)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain-community) (0.1.82)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain-community) (1.22.0)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain-community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain-community) (8.4.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/name/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/name/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/name/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/name/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/name/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/name/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/envs/name/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/envs/name/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain<0.3.0,>=0.2.6->langchain-community) (0.2.2)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain<0.3.0,>=0.2.6->langchain-community) (2.7.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain-community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/envs/name/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain-community) (24.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/envs/name/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/name/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/name/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/name/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (2.2.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/name/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (2024.6.2)\nRequirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/envs/name/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/name/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/name/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/name/lib/python3.9/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.18.4 in /opt/conda/envs/name/lib/python3.9/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain-community) (2.18.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/name/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"%%writefile main.py\nimport torch\nfrom pydub import AudioSegment\nfrom diffusers import StableDiffusionXLPipeline, DPMSolverSinglestepScheduler\nfrom langchain_community.llms import Ollama\nfrom langchain import PromptTemplate, LLMChain\nfrom moviepy.editor import *\nfrom TTS.api import TTS\nimport whisper\nimport os\nimport json\nimport ffmpeg\nfrom PIL import Image, ImageDraw, ImageFont\nimport pandas as pd  \nfrom pytrends.request import TrendReq\n\n# Initialize models and pipeline\ndef initialize_models():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Load stable diffusion pipeline\n    pipe = StableDiffusionXLPipeline.from_pretrained(\"sd-community/sdxl-flash\", torch_dtype=torch.float16).to(device)\n    pipe.scheduler = DPMSolverSinglestepScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n\n    # Initialize TTS model\n    tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC_ph\", progress_bar=False).to(device)\n\n    return pipe, tts, device\n\npipe, tts, device = initialize_models()\n\ndef get_trend_keyword():\n    # Initialize pytrends\n    pytrends = TrendReq(hl='en-US', tz=360)\n    \n    # Get trending searches\n    trending_searches_df = pytrends.trending_searches(pn='united_states') # You can change the region\n    \n    trending_searches_df.head()\n    \n    # Get the top trending keyword\n    top_trending_keyword = trending_searches_df.iloc[0, 0]\n    \n    print(\"Top trending keyword on YouTube:\", top_trending_keyword)\n    \n    return top_trending_keyword\n\n# LLM initialization\ndef get_llm(model_name):\n    print(f\"Building LLM Model: {model_name}\")\n    return Ollama(model=model_name, temperature=0.7)\n\n# Prompt templates\ndef build_template():\n    fact_template = '''\nYou are a talented YouTube content creator working for a famous YouTuber. Your task is to create a 35-second YouTube Short script on \"Top 3 facts {topic}\" for a wide audience. Start with \"Here are the Top 3 facts {topic}. The last one will blow your mind.\" Present the topics in descending order from 3rd to 1st. Be creative and engaging.\n\nClose the script by asking viewers to like, subscribe, and share the video.\n\nExample:\n\nTitle: Top 5 Mysterious Facts About Ancient Indian Men\nDescription: Intriguing facts about ancient Indian men.\nVisual: A statue of an ancient Indian man in the city.\nNarration: \"Here are the Top 5 Mysterious Facts About Ancient Indian Men. The last one will blow your mind!\"\nVisual: Ancient Indian men doing yoga, transitioning to a modern yoga class.\nNarration: \"Fact 5: Yoga, practiced by millions today, was created by ancient Indian men thousands of years ago.\"\nVisual: An ancient Indian man observing the night sky, transitioning to a modern planetarium.\nNarration: \"Fact 4: Ancient Indian men were stellar astronomers who calculated the Earth's circumference and theorized about gravity centuries before Western science.\"\nVisual: Ruins of the Indus Valley Civilization with undeciphered scripts.\nNarration: \"Fact 3: The Indus Valley Civilization's script remains undeciphered. Their urban planning and drainage systems are still a mystery.\"\nVisual: Ancient Ayurvedic texts, transitioning to modern herbal medicine.\nNarration: \"Fact 2: Ayurveda, one of the oldest holistic healing systems, was pioneered by ancient Indian men. Their texts detail surgical procedures and herbal medicines.\"\nVisual: Artistic depictions of Vimanas flying, transitioning to ancient scriptures and modern interpretations.\nNarration: \"Fact 1: The Vimana, described in ancient texts as flying machines, suggest advanced technology or extraterrestrial contact.\"\nVisual: Colorful bubbles containing colorful youtube, like and share buttons.\nNarration: \"If these facts blew your mind, like, subscribe, and share this video. See you next time!\"\n\nCreate your script following this format. Keep it short, interesting, and simple.\n    '''  # Same as in the original\n    quiz_template = '''\nYou are a talented YouTube content creator working for a renowned Investigation Quiz Creator. Your task is to craft a 55-second YouTube Short script focused on an intriguing investigation quiz {topic}.\n\nSet the Scene: Introduce the crime and the setting.\nIntroduce Suspects: Present each suspect with a brief description.\nPresent Clues: Show compelling evidence.\nInvite Interaction: Ask viewers to identify the culprit in the comments.\nCall to Action: Encourage viewers to like, subscribe, and share.\n\nExample 1:\n\nTitle: Quick Investigation Quiz: Who Stole the Diamond Necklace?\nDescription: Can you solve the mystery in 55 seconds?\nVisual: A detective's magnifying glass focusing on a diamond necklace.\nNarration: \"Hey detectives! Ready for a quick investigation quiz? Let's see if you can crack the case in 55 seconds!\"\nVisual: A hotel room in disarray, with a broken window illustrating the crime scene.\nNarration: \"Here's the scene: A valuable diamond necklace has gone missing from this hotel room. The room was ransacked, and the window was broken from the inside. There are three suspects who were in the hotel at the time.\"\nVisual: Mr. White, looking concerned.\nNarration: \"Suspect A: Mr. White, the hotel manager, who has access to all rooms.\"\nVisual: Ms. Blue, looking sly.\nNarration: \"Suspect B: Ms. Blue, a famous jewel thief recently spotted in the area.\"\nVisual: Mr. Red, looking nervous.\nNarration: \"Suspect C: Mr. Red, a guest staying in the room next door, known for his shady dealings.\"\nVisual: An illustration of some evidence boxes in the police locker.\nNarration: \"Here are the clues:\"\nVisual: A glove on the floor, with a magnifying glass hovering over it.\nNarration: \"Clue 1: A glove was found on the floor, but it doesn't belong to the room's owner.\"\nVisual: A key card in the hallway.\nNarration: \"Clue 2: A room key card was found in the hallway, registered to Mr. Red.\"\nVisual: Security camera footage showing Ms. Blue entering the hotel.\nNarration: \"Clue 3: A security camera shows Ms. Blue entering the hotel at the time of the theft.\"\nVisual: The three suspects (i.e., Mr. White, Ms. Blue, and Mr. Red) side by side.\nNarration: \"Who do you think stole the diamond necklace? Was it Mr. White, Ms. Blue, or Mr. Red? Comment your answer below!\"\nVisual: YouTube's Like and Share buttons.\nNarration: \"Don't forget to like, share, and subscribe for more investigation quizzes!\"\n\nExample 2:\n\nTitle: Quick Investigation Quiz: Who Sabotaged the Space Mission?\nDescription: Can you solve the mystery in 55 seconds?\nVisual: A detective's magnifying glass focusing on a space shuttle model with a red \"X.\"\nNarration: \"Hey space sleuths! Ready for a quick investigation quiz? Let's see if you can crack the case in 55 seconds!\"\nVisual: A high-tech control room in chaos, with monitors flickering and alarms blaring.\nNarration: \"Here's the scene: The launch of the highly anticipated space mission was sabotaged. The control room is in disarray, and three people were present during the incident.\"\nVisual: Commander Red, looking determined.\nNarration: \"Suspect A: Commander Red, the mission leader, known for his strict discipline.\"\nVisual: Dr. Silver, looking secretive.\nNarration: \"Suspect B: Dr. Silver, the lead scientist, who has been acting suspiciously lately.\"\nVisual: Engineer Green, looking nervous.\nNarration: \"Suspect C: Engineer Green, responsible for the shuttle's systems, who was seen leaving the control room in a hurry.\"\nVisual: An illustration of some evidence in a mission locker.\nNarration: \"Here are the clues:\"\nVisual: A high-tech glove with burnt fingertips on the control panel.\nNarration: \"Clue 1: A high-tech glove with burnt fingertips was found on the control panel.\"\nVisual: A cryptic note with a space-themed riddle found in the trash can.\nNarration: \"Clue 2: A cryptic note with a space-themed riddle was found in the trash can, hinting at the sabotage.\"\nVisual: Security footage showing Engineer Green handling a mysterious glowing vial.\nNarration: \"Clue 3: Security footage shows Engineer Green handling a mysterious glowing vial right before the sabotage.\"\nVisual: The three suspects (i.e., Commander Red, Dr. Silver, and Engineer Green) side by side.\nNarration: \"Who do you think sabotaged the space mission? Was it Commander Red, Dr. Silver, or Engineer Green? Comment your answer below!\"\nVisual: YouTube's Like and Share buttons.\nNarration: \"Don't forget to like, share, and subscribe for more investigation quizzes!\"\n\nExample 3:\n\nTitle: Quick Investigation Quiz: Who Stole the Golden Trophy?\nDescription: Can you solve the mystery in 55 seconds?\nVisual: A detective's magnifying glass focusing on a golden trophy on a pedestal.\nNarration: \"Hey detectives! Ready for a quick investigation quiz? Let's see if you can crack the case in 55 seconds!\"\nVisual: A grand ballroom in disarray, with overturned chairs and scattered decorations illustrating the crime scene.\nNarration: \"Here's the scene: During a prestigious awards ceremony, the golden trophy went missing. The ballroom is in chaos, and three people were nearby when it happened.\"\nVisual: Ms. Scarlet, looking worried.\nNarration: \"Suspect A: Ms. Scarlet, the event organizer, who was seen near the trophy just before it vanished.\"\nVisual: Mr. Plum, looking suspicious.\nNarration: \"Suspect B: Mr. Plum, a rival contestant, known for his jealousy over not winning.\"\nVisual: Mrs. Peacock, looking composed.\nNarration: \"Suspect C: Mrs. Peacock, a famous journalist, who was interviewing guests near the trophy.\"\nVisual: An illustration of some evidence in a police locker.\nNarration: \"Here are the clues:\"\nVisual: A broken watch with a red band found near the pedestal.\nNarration: \"Clue 1: A broken watch with a distinctive red band was found near the pedestal.\"\nVisual: A smudged fingerprint on the trophy's case.\nNarration: \"Clue 2: A smudged fingerprint was found on the trophy's case, but it's unclear whose it is.\"\nVisual: Security footage showing Mrs. Peacock hurriedly leaving the ballroom with a bag.\nNarration: \"Clue 3: Security footage shows Mrs. Peacock hurriedly leaving the ballroom with a bag shortly after the trophy disappeared.\"\nVisual: The three suspects (i.e., Ms. Scarlet, Mr. Plum, and Mrs. Peacock) side by side.\nNarration: \"Who do you think stole the golden trophy? Was it Ms. Scarlet, Mr. Plum, or Mrs. Peacock? Comment your answer below!\"\nVisual: YouTube's Like and Share buttons.\nNarration: \"Don't forget to like, share, and subscribe for more investigation quizzes!\"\n\nCraft your script following this structure. Keep it concise, intriguing, and engaging for your audience.\n    '''  # Same as in the original\n    \n    seo_template = '''\n    Write an engaging and SEO-friendly title and description for a youtube shorts based on the below given contect that can drive traffic to my video.\n    Strictly include \"#shorts\", \"#trending\" hashtags in the title. Moreover, include some seo-friendly hashtags in the description as well.\n    \n    Content:\n    \n    {response}\n    '''\n    return quiz_template, fact_template, seo_template\n\n# LLM chain\ndef get_llm_chain(template, llm):\n    print(\"Building LLM Chain with prompt and model\")\n    prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n    return prompt | llm\n\n# LLM chain\ndef get_seo_llm_chain(template, llm):\n    print(\"Building LLM Chain with prompt and model\")\n    prompt = PromptTemplate(template=template, input_variables=[\"response\"])\n    return prompt | llm\n\n# Parsing LLM response\ndef parse_llm_response(response):\n    print(\"Parsing response from LLM model\")\n    text_array = response.splitlines()\n\n    title = \"\"\n    description = \"\"\n    visuals = []\n    narrations = []\n\n    for text in text_array:\n        if \"Title:\" in text:\n            title = text.replace(\"Title: \", \"\")\n        elif \"Description:\" in text:\n            description = text.replace(\"Description: \", \"\")\n        elif \"Visual:\" in text:\n            visuals.append(text.replace(\"Visual: \", \"\"))\n        elif \"Narration:\" in text:\n            narrations.append(text.replace(\"Narration: \", \"\"))\n\n    return title, description, visuals, narrations\n\n# Convert text to images\ndef convert_text_to_image(visuals, output_dir='images'):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    print(\"Converting text to images\")\n    image_files = []\n    for count, visual in enumerate(visuals):\n        file_path = os.path.join(output_dir, f'output_image{count}.png')\n        pipe(f\"{visual}, realism\", width=768, height=1344, num_inference_steps=7, guidance_scale=3).images[0].save(file_path)\n        image_files.append(file_path)\n    \n    return image_files\n\n# Change audio speed\ndef change_speed(audio, speed_factor):\n    return audio._spawn(audio.raw_data, overrides={\"frame_rate\": int(audio.frame_rate * speed_factor)}).set_frame_rate(audio.frame_rate)\n\n# Generate speech from text\ndef generate_speech(narrations, speed_factor=1.25, output_dir='speeches'):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    audio_files = []\n    for count, narration in enumerate(narrations):\n        file_name = os.path.join(output_dir, f\"speech{count}.wav\")\n        tts.tts_to_file(text=narration, file_path=file_name)\n        \n        audio = AudioSegment.from_file(file_name)\n        audio = change_speed(audio, speed_factor)\n        audio.export(file_name, format=\"wav\")\n        \n        audio_files.append(file_name)\n    \n    return audio_files\n\n# Generate video\ndef generate_video(image_files, audio_files, output_file):\n    video_clips = []\n    for image, audio in zip(image_files, audio_files):\n        audio_clip = AudioFileClip(audio)\n        img_clip = ImageClip(image).set_duration(audio_clip.duration).set_audio(audio_clip)\n        video_clips.append(img_clip)\n    \n    final_video = concatenate_videoclips(video_clips, method=\"compose\")\n    final_video = final_video.set_fps(24)\n    \n    final_audio = final_video.audio\n    final_audio.fps = 44100\n    audio_file_path = 'output_audio.mp3'\n    final_audio.write_audiofile(audio_file_path)\n    \n    transcribe_audio_with_timestamps(audio_file_path)\n    \n    line_level_timestamps = split_text_into_lines()\n    \n    create_subtitle_images_with_highlight(line_level_timestamps, 768, 1344)\n    \n    final_video = add_highlighted_subtitles(final_video, line_level_timestamps)\n    \n    final_video = generate_video_with_bg_audio(final_video, \"/kaggle/input/prisms/prism.mp3\")\n    \n    final_video.write_videofile(output_file, codec=\"libx264\", audio_codec=\"aac\")\n\n\n# Transcribe audio with timestamps\ndef transcribe_audio_with_timestamps(audiofilename):\n    model = whisper.load_model(\"medium\", device=device)\n    result = model.transcribe(audiofilename, word_timestamps=True)\n    \n    wordlevel_info = [{'word': word['word'].strip(), 'start': word['start'], 'end': word['end']} for segment in result['segments'] for word in segment['words']]\n    \n    with open('data.json', 'w') as f:\n        json.dump(wordlevel_info, f, indent=4)\n\n# Split text into lines for subtitles\ndef split_text_into_lines():\n    with open('data.json', 'r') as f:\n        data = json.load(f)\n    \n    MaxChars = 10 \n    MaxDuration = 1\n    MaxGap = 1.5\n\n    subtitles = []\n    line = []\n    line_duration = 0\n\n    for idx, word_data in enumerate(data):\n        word, start, end = word_data[\"word\"], word_data[\"start\"], word_data[\"end\"]\n        line.append(word_data)\n        line_duration += end - start\n        temp = \" \".join(item[\"word\"] for item in line)\n\n        duration_exceeded = line_duration > MaxDuration \n        chars_exceeded = len(temp) > MaxChars \n        maxgap_exceeded = idx > 0 and word_data['start'] - data[idx-1]['end'] > MaxGap\n\n        if duration_exceeded or chars_exceeded or maxgap_exceeded:\n            if line:\n                subtitles.append({\n                    \"word\": \" \".join(item[\"word\"] for item in line),\n                    \"start\": line[0][\"start\"],\n                    \"end\": line[-1][\"end\"],\n                    \"textcontents\": line\n                })\n                line = []\n                line_duration = 0\n\n    if line:\n        subtitles.append({\n            \"word\": \" \".join(item[\"word\"] for item in line),\n            \"start\": line[0][\"start\"],\n            \"end\": line[-1][\"end\"],\n            \"textcontents\": line\n        })\n\n    return subtitles\n\n# Create subtitle images\ndef create_subtitle_image(text, current_word_idx, width, height, font_path='/kaggle/input/boldttf/Roboto-Bold.ttf', font_size=80):\n    img = Image.new('RGBA', (width, height), (0, 0, 0, 0))\n    draw = ImageDraw.Draw(img)\n    \n    try:\n        font = ImageFont.truetype(font_path, font_size) if font_path else ImageFont.load_default()\n    except Exception as e:\n        print(f\"Error loading font: {e}. Using default font.\")\n        font = ImageFont.load_default()\n    \n    words = text.split()\n    total_width = sum([draw.textbbox((0, 0), word, font=font)[2] for word in words])\n    x_offset = (width - total_width) // 2\n    \n    outline_offset = 2\n\n    for idx, word in enumerate(words):\n        fill = (255, 0, 0, 255) if idx == current_word_idx else (255, 255, 255, 255)\n        word_bbox = draw.textbbox((0, 0), word, font=font)\n        word_width = word_bbox[2] - word_bbox[0]\n        word_height = word_bbox[3] - word_bbox[1]\n\n        for x in range(-outline_offset, outline_offset + 1):\n            for y in range(-outline_offset, outline_offset + 1):\n                if x != 0 or y != 0:\n                    draw.text((x_offset + x, (height - word_height) // 2 + y), word, font=font, fill=(0, 0, 0, 255))\n        \n        draw.text((x_offset, (height - word_height) // 2), word, font=font, fill=fill)\n        x_offset += word_width + draw.textbbox((0, 0), ' ', font=font)[2]\n    \n    return img\n\ndef create_subtitle_images_with_highlight(linelevel_timestamps, width, height, output_dir='subtitles'):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    for i, line in enumerate(linelevel_timestamps):\n        subtitle = line['word']\n        for j, content in enumerate(line['textcontents']):\n            img = create_subtitle_image(subtitle, j, width, height)\n            img_path = os.path.join(output_dir, f\"subtitle_{i}_{j}.png\")\n            img.save(img_path)\n\n# Add highlighted subtitles to video\ndef add_highlighted_subtitles(video, linelevel_timestamps, input_dir='subtitles'):\n    subclips = []\n\n    for i, line in enumerate(linelevel_timestamps):\n        for j, content in enumerate(line['textcontents']):\n            img_path = os.path.join(input_dir, f\"subtitle_{i}_{j}.png\")\n            if not os.path.exists(img_path):\n                print(f\"Subtitle image {img_path} does not exist.\")\n                continue\n\n            img_clip = ImageClip(img_path).set_start(content['start']).set_duration(content['end'] - content['start']).set_position((\"center\", \"bottom\"))\n            subclips.append(img_clip)\n\n    video_with_subs = CompositeVideoClip([video, *subclips])\n    return video_with_subs\n\ndef generate_video_with_bg_audio(video_clip, bg_file_path):\n    # Path to your audio file\n    audio_path = bg_file_path\n    \n    # Duration of the video clip in seconds\n    video_duration = video_clip.duration\n    \n    # Load audio clip\n    audio_clip = AudioFileClip(audio_path)\n    \n    # Start the audio from 25 seconds and end at the end of the video clip\n    start_time = 25  # seconds\n    end_time = start_time + video_duration  # seconds\n    \n    # Trim the audio clip\n    trimmed_audio = audio_clip.subclip(start_time, end_time)\n    \n    # Set the volume of the background music (audio from audio file)\n    background_music_volume = 0.15  # You can adjust this value as needed\n    \n    # Set the volume of the original video's audio\n    original_audio_volume = 1.0  # Full volume (1.0)\n    \n    # Set the volume for the background music\n    background_music = trimmed_audio.volumex(background_music_volume)\n    \n    # Get original video's audio\n    original_audio = video_clip.audio\n    \n    # Reduce the volume of the original video's audio\n    reduced_original_audio = original_audio.volumex(original_audio_volume - background_music_volume)\n    \n    # Combine the reduced original audio with the background music\n    final_audio = CompositeAudioClip([reduced_original_audio, background_music])\n    \n    # Set the audio of the video clip to the combined audio\n    video_clip = video_clip.set_audio(final_audio)\n    return video_clip\n\n# Main function to generate video with subtitles and background music\ndef main(model_name, niche, topic):\n    \n    llm = get_llm(model_name)\n    \n    quiz_template, fact_template, seo_template = build_template()\n    template = quiz_template if niche == \"quiz\" else fact_template\n    \n    llm_chain = get_llm_chain(template, llm)\n\n    response = llm_chain.invoke(topic)\n    \n    seo_llm_chain = get_seo_llm_chain(seo_template, llm)\n    seo_response = seo_llm_chain.invoke(response)\n    print(seo_response)\n    \n    title, description, visuals, narrations = parse_llm_response(response)\n\n    image_files = convert_text_to_image(visuals)\n    audio_files = generate_speech(narrations)\n\n    output_file = 'output_video.mp4'\n    generate_video(image_files, audio_files, output_file)\n\n# Execute main function\nif __name__ == \"__main__\":\n    model_name, niche, topic = sys.argv[1:]\n    \n    trend_keyword = get_trend_keyword()\n    \n    yt_topic = trend_keyword if topic.lower() == \"trend\" else topic\n    \n    if topic != \"\":\n        topic = f\"about {yt_topic}\"\n    else:\n        topic = \"about a random topic\"\n    main(model_name, niche, topic)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T13:04:54.269136Z","iopub.execute_input":"2024-06-30T13:04:54.269458Z","iopub.status.idle":"2024-06-30T13:04:54.289325Z","shell.execute_reply.started":"2024-06-30T13:04:54.269433Z","shell.execute_reply":"2024-06-30T13:04:54.288511Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Overwriting main.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ./main.py \"mistral:7b-instruct-fp16\" \"quiz\" \"trend\"","metadata":{"execution":{"iopub.status.busy":"2024-06-30T13:05:01.591643Z","iopub.execute_input":"2024-06-30T13:05:01.592700Z","iopub.status.idle":"2024-06-30T13:12:55.390782Z","shell.execute_reply.started":"2024-06-30T13:05:01.592657Z","shell.execute_reply":"2024-06-30T13:12:55.389843Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"/opt/conda/envs/name/lib/python3.9/site-packages/langchain/__init__.py:29: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n  warnings.warn(\n/opt/conda/envs/name/lib/python3.9/site-packages/langchain/__init__.py:29: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n  warnings.warn(\nLoading pipeline components...: 100%|██████████| 7/7 [00:04<00:00,  1.41it/s]\ntime=2024-06-30T13:06:01.397Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.required.full=\"14.5 GiB\" memory.required.partial=\"14.5 GiB\" memory.required.kv=\"256.0 MiB\" memory.required.allocations=\"[14.5 GiB]\" memory.weights.total=\"13.3 GiB\" memory.weights.repeating=\"13.0 GiB\" memory.weights.nonrepeating=\"250.0 MiB\" memory.graph.full=\"164.0 MiB\" memory.graph.partial=\"181.0 MiB\"\ntime=2024-06-30T13:06:01.405Z level=INFO source=server.go:368 msg=\"starting llama server\" cmd=\"/tmp/ollama128773846/runners/cuda_v11/ollama_llama_server --model /root/.ollama/models/blobs/sha256-bbd75c7dc4292c6106265222c6eeddaaeaec9ea5a54babb6a81bce4799fcaffd --ctx-size 2048 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --parallel 1 --port 34607\"\ntime=2024-06-30T13:06:01.405Z level=INFO source=sched.go:382 msg=\"loaded runners\" count=1\ntime=2024-06-30T13:06:01.405Z level=INFO source=server.go:556 msg=\"waiting for llama runner to start responding\"\ntime=2024-06-30T13:06:01.406Z level=INFO source=server.go:594 msg=\"waiting for server to become available\" status=\"llm server error\"\nllama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-bbd75c7dc4292c6106265222c6eeddaaeaec9ea5a54babb6a81bce4799fcaffd (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = mistralai\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 1\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","output_type":"stream"},{"name":"stdout","text":"INFO [main] build info | build=1 commit=\"7c26775\" tid=\"134743960854528\" timestamp=1719752761\nINFO [main] system info | n_threads=2 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"134743960854528\" timestamp=1719752761 total_threads=4\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"3\" port=\"34607\" tid=\"134743960854528\" timestamp=1719752761\n","output_type":"stream"},{"name":"stderr","text":"llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = [\"▁ t\", \"i n\", \"e r\", \"▁ a\", \"h e...\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special tokens cache size = 259\nllm_load_vocab: token to piece cache size = 0.1637 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 13.49 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = mistralai\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\nggml_cuda_init: CUDA_USE_TENSOR_CORES: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\ntime=2024-06-30T13:06:01.658Z level=INFO source=server.go:594 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllm_load_tensors: ggml ctx size =    0.30 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   250.00 MiB\nllm_load_tensors:      CUDA0 buffer size = 13563.02 MiB\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB\nllama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.14 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    12.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\n","output_type":"stream"},{"name":"stdout","text":"INFO [main] model loaded | tid=\"134743960854528\" timestamp=1719752797\n","output_type":"stream"},{"name":"stderr","text":"time=2024-06-30T13:06:38.072Z level=INFO source=server.go:599 msg=\"llama runner started in 36.67 seconds\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/06/30 - 13:07:15 | 200 |         1m14s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/06/30 - 13:07:29 | 200 |  14.43475097s |       127.0.0.1 | POST     \"/api/generate\"\n","output_type":"stream"},{"name":"stderr","text":"Changing scheduler {self.config} to have `lower_order_final` set to True to handle uneven amount of inference steps. Please make sure to always use an even number of `num_inference steps when using `lower_order_final=False`.\n  0%|          | 0/7 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" > tts_models/en/ljspeech/tacotron2-DDC_ph is already downloaded.\n > vocoder_models/en/ljspeech/univnet is already downloaded.\n > Using model: Tacotron2\n > Setting up Audio Processor...\n | > sample_rate:22050\n | > resample:False\n | > num_mels:80\n | > log_func:np.log10\n | > min_level_db:-100\n | > frame_shift_ms:None\n | > frame_length_ms:None\n | > ref_level_db:20\n | > fft_size:1024\n | > power:1.5\n | > preemphasis:0.0\n | > griffin_lim_iters:60\n | > signal_norm:True\n | > symmetric_norm:True\n | > mel_fmin:50.0\n | > mel_fmax:7600.0\n | > pitch_fmin:0.0\n | > pitch_fmax:640.0\n | > spec_gain:1.0\n | > stft_pad_mode:reflect\n | > max_norm:4.0\n | > clip_norm:True\n | > do_trim_silence:True\n | > trim_db:60\n | > do_sound_norm:False\n | > do_amp_to_db_linear:True\n | > do_amp_to_db_mel:True\n | > do_rms_norm:False\n | > db_level:None\n | > stats_path:/root/.local/share/tts/tts_models--en--ljspeech--tacotron2-DDC_ph/scale_stats.npy\n | > base:10\n | > hop_length:256\n | > win_length:1024\n > Model's reduction rate `r` is set to: 2\n > Vocoder Model: univnet\n > Setting up Audio Processor...\n | > sample_rate:22050\n | > resample:False\n | > num_mels:80\n | > log_func:np.log10\n | > min_level_db:-100\n | > frame_shift_ms:None\n | > frame_length_ms:None\n | > ref_level_db:20\n | > fft_size:1024\n | > power:1.5\n | > preemphasis:0.0\n | > griffin_lim_iters:60\n | > signal_norm:True\n | > symmetric_norm:True\n | > mel_fmin:50.0\n | > mel_fmax:7600.0\n | > pitch_fmin:1.0\n | > pitch_fmax:640.0\n | > spec_gain:1.0\n | > stft_pad_mode:reflect\n | > max_norm:4.0\n | > clip_norm:True\n | > do_trim_silence:True\n | > trim_db:60\n | > do_sound_norm:False\n | > do_amp_to_db_linear:True\n | > do_amp_to_db_mel:True\n | > do_rms_norm:False\n | > db_level:None\n | > stats_path:/root/.local/share/tts/vocoder_models--en--ljspeech--univnet/scale_stats.npy\n | > base:10\n | > hop_length:256\n | > win_length:1024\n > Generator Model: univnet_generator\n > Discriminator Model: univnet_discriminator\nTop trending keyword on YouTube: UFC 303\nBuilding LLM Model: mistral:7b-instruct-fp16\nBuilding LLM Chain with prompt and model\nBuilding LLM Chain with prompt and model\nTitle: UFC 303 Investigation Quiz: Who Staged the Fight? #shorts #trending\nDescription: Solve the mystery in just 55 seconds! Who staged UFC 303? Three suspects were present during the incident, including Mr. Black, Ms. White, and Mr. Green. Check out the clues and see if you can crack the case! Clue 1: A torn piece of fabric was found near the octagon, suggesting someone tampered with the equipment. Clue 2: A mysterious note with a cryptic message was found in the locker room, hinting at the staging. Clue 3: Security footage shows Mr. Green receiving a bribe from an unknown source right before the fight began. Who do you think is responsible? Comment below! #investigationquiz #ufc303 #mixedmartialarts #staging #suspectA #suspectB #suspectC #detective #crime #clues #mystery\nParsing response from LLM model\nConverting text to images\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7/7 [00:06<00:00,  1.16it/s]\n100%|██████████| 7/7 [00:04<00:00,  1.40it/s]\n100%|██████████| 7/7 [00:05<00:00,  1.40it/s]\n100%|██████████| 7/7 [00:05<00:00,  1.38it/s]\n100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\n100%|██████████| 7/7 [00:05<00:00,  1.35it/s]\n100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\n100%|██████████| 7/7 [00:05<00:00,  1.38it/s]\n100%|██████████| 7/7 [00:05<00:00,  1.39it/s]\n100%|██████████| 7/7 [00:05<00:00,  1.39it/s]\n100%|██████████| 7/7 [00:04<00:00,  1.41it/s]\n","output_type":"stream"},{"name":"stdout","text":" > Text splitted to sentences.\n['\"Hey fight fans! Ready for a quick investigation quiz? Let\\'s see if you can crack the case in 55 seconds!\"']\n > Processing time: 1.6624391078948975\n > Real-time factor: 0.24063112021506727\n > Text splitted to sentences.\n['\"Here\\'s the scene: The main event of UFC 303 was staged, and the arena is in disarray. Three fighters were present during the incident.\"']\nhɪɹz ðə sin, ðə meɪn ɪvɛnt əv ju ɛf si θɹi hʌndɹɪd θɹi wəz steɪd͡ʒd, ænd ðə ɚinə ɪz ɪn dɪsəɹeɪ. θɹi faɪtɚz wɚ pɹɛzənt dʊɹɪŋ ðə ɪnsɪdənt.\n [!] Character '͡' not found in the vocabulary. Discarding it.\n > Processing time: 0.9596095085144043\n > Real-time factor: 0.09897925708565329\n > Text splitted to sentences.\n['\"Suspect A: Mr. Black, the promoter, who had a financial stake in the fight\\'s outcome.\"']\n > Processing time: 0.47475314140319824\n > Real-time factor: 0.0914677999435597\n > Text splitted to sentences.\n['\"Suspect B: Ms. White, the lead fighter, who was supposed to win the bout but lost unexpectedly.\"']\n > Processing time: 0.5964546203613281\n > Real-time factor: 0.0875108084409087\n > Text splitted to sentences.\n['\"Suspect C: Mr. Green, the referee, who made a controversial call during the fight.\"']\n > Processing time: 0.4481937885284424\n > Real-time factor: 0.07188025891024784\n > Text splitted to sentences.\n['\"Here are the clues:\"']\n > Processing time: 0.22328615188598633\n > Real-time factor: 0.12244975251407676\n > Text splitted to sentences.\n['\"Clue 1: A torn piece of fabric was found near the octagon, suggesting that someone tampered with the equipment.\"']\n > Processing time: 0.657867431640625\n > Real-time factor: 0.0939603642066262\n > Text splitted to sentences.\n['\"Clue 2: A mysterious note with a cryptic message was found in the locker room, hinting at the staging.\"']\n > Processing time: 0.5689826011657715\n > Real-time factor: 0.08434216921927276\n > Text splitted to sentences.\n['\"Clue 3: Security footage shows Mr. Green receiving a bribe from an unknown source right before the fight began.\"']\n > Processing time: 0.6958789825439453\n > Real-time factor: 0.08132012404124264\n > Text splitted to sentences.\n['\"Who do you think staged UFC 303? Was it Mr. Black, Ms. White, or Mr. Green? Comment your answer below!\"']\n > Processing time: 0.6392107009887695\n > Real-time factor: 0.07808813467778991\n > Text splitted to sentences.\n['\"Don\\'t forget to like, share, and subscribe for more investigation quizzes!\"']\n > Processing time: 0.4362204074859619\n > Real-time factor: 0.08676089609851223\nMoviePy - Writing audio in output_audio.mp3\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nMoviepy - Building video output_video.mp4.\nMoviePy - Writing audio in output_videoTEMP_MPY_wvf_snd.mp4\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nMoviepy - Writing video output_video.mp4\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Moviepy - Done !\nMoviepy - video ready output_video.mp4\n","output_type":"stream"},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"!pip install --upgrade youtube-uploader-selenium","metadata":{"execution":{"iopub.status.busy":"2024-06-30T12:00:05.326055Z","iopub.execute_input":"2024-06-30T12:00:05.326377Z","iopub.status.idle":"2024-06-30T12:00:09.806196Z","shell.execute_reply.started":"2024-06-30T12:00:05.326352Z","shell.execute_reply":"2024-06-30T12:00:09.805187Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Requirement already satisfied: youtube-uploader-selenium in /opt/conda/envs/name/lib/python3.9/site-packages (0.1.0)\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"%%writefile youtube.py\nfrom youtube_uploader_selenium import YouTubeUploader\n\nvideo_path = '/output_video.mp4'\nmetadata_path = '/.json'\n\nuploader = YouTubeUploader(video_path, metadata_path, thumbnail_path)\nwas_video_uploaded, video_id = uploader.upload()\nassert was_video_uploaded","metadata":{"execution":{"iopub.status.busy":"2024-06-30T12:00:09.807333Z","iopub.execute_input":"2024-06-30T12:00:09.807634Z","iopub.status.idle":"2024-06-30T12:00:09.814265Z","shell.execute_reply.started":"2024-06-30T12:00:09.807607Z","shell.execute_reply":"2024-06-30T12:00:09.813187Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Overwriting youtube.py\n","output_type":"stream"}]}]}